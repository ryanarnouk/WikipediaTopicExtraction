{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wikipedia Dataset Topic Extraction",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohs1hbHDKVcI",
        "colab_type": "text"
      },
      "source": [
        "# Simple Wikipedia Dataset Topic Extraction Using LDA\n",
        "Ryan Arnouk\n",
        "\n",
        "Latent Dirichlet Allocation: http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye9z8-d4o5fK",
        "colab_type": "text"
      },
      "source": [
        "## Latent Dirichlet Allocation (LDA) in Topic Modelling\n",
        "### Explanation\n",
        "In natural language processing (NLP) latent Dirichlet allocation is a generative statistical model. This means the model can extract topics in an unlabeled dataset by frequency and distribution of words, to extract topics. \n",
        "\n",
        "LDA assumes all the words in a document are related and tries to figure out how each document could have been created. We need to just tell the model how many topics to construct to generate topic and word distributions over a corpus. Using this can allow us to identify similar documents within the corpus. \n",
        "\n",
        "Meaning, LDA Is useful for finding accurate mixtures of topics within a document. \n",
        "\n",
        "When using LDA you would choose a fixed number of topics (k) to discover from the dataset. In my case, I chose 200 to represent all the topics from the Simple Wikipedia Dataset. \n",
        "\n",
        "### Assumptions\n",
        "- Documents with similar topics will use similar groups of words\n",
        "- Document definitions/modeling: \n",
        "  - Documents are probability distributions over latent topics\n",
        "  - Topics are probability distributions over words. \n",
        "\n",
        "Instead of focusing purely on frequency of words in a topic LDA focuses also on the distribution between words accross topics.  \n",
        "\n",
        "\n",
        "### Plate Notation\n",
        "A simple way to visually represent all the dependencies in the models parameters: \n",
        "\n",
        "![Plate Notation](https://upload.wikimedia.org/wikipedia/commons/d/d3/Latent_Dirichlet_allocation.svg)\n",
        "\n",
        "By Bkkbrad - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=3610403\n",
        "\n",
        "\n",
        "M represents total number of documents within corpus \n",
        "\n",
        "N denotes number of words in a document\n",
        "\n",
        "Outside parameters: \n",
        "Dirchilet priors\n",
        "- α = the parameter of the Dirichlet prior on the per-document topic distributions\n",
        "    - High alpha denotes higher likelihood that each document is going to contain a mixture of the documents. \n",
        "    - Low alphas suggest that each document will only contain 1 or 2 topics\n",
        "- β = is the parameter of the Dirichlet prior on the per-topic word distribution\n",
        "    - High beta means each topic will contain a mixture of most of the words\n",
        "    - Low beta means each topic will only contain a mixture of a few words.      \n",
        "- Theta: Topic distribution for the document \n",
        "- z notates each topic, making each document a mixture of these topics. \n",
        "- w stands for word \n",
        "\n",
        "<img src=\"https://miro.medium.com/max/800/1*_NdnljMqi8L2_lAYwH3JDQ.gif\" alt=\"Dirichlet Distribution\" width=\"500px\" height=\"500px\"/>\n",
        "\n",
        "1000 samples from a Dirichlet distribution using an increasing alpha value.\n",
        "\n",
        "### How LDA Works\n",
        "**LDA Works Backwards**\n",
        "\n",
        "LDA runs in reverse, by starting with a corpus of documents and generating topics. In order to understand how LDA assumes the documents topics it is important to understand the generative process: \n",
        "\n",
        "LDA assumes documents are created in the same way: \n",
        "1. Determine number of words in document\n",
        "2. Choose a topic mixture for the document over a fixed set of topics (k)\n",
        "  a. Topic A: 20%\n",
        "  b. Topic B: 20%\n",
        "  c. Topic C: 60%\n",
        "3. Generate topics in a document by: \n",
        "  a. First pick a topic a document in the topic distribution above. \n",
        "  b. Pick a word based on the topic distribution. \n",
        "\n",
        "Since we have a corpus of documents, we want LDA to learn the topic representation of K topics in each document and the word distribution of each topic. \n",
        "\n",
        "LDA **backtracks** from the document level to identify topics that have likely generated the corpus. \n",
        "\n",
        "Steps: \n",
        "- Randomly assign each word in each document to one of the K topics. \n",
        "- For each document d: \n",
        "  - Assume all the topic assignments except the current one are correct\n",
        "  - Calculate two proportions: \n",
        "    - Proportion of words in document d that are currently assigned to topic t \n",
        "    - Proportion of assignments to topic t over all documents that come from this word. \n",
        "\n",
        "  - Multiply the two proportions and assign w a new topic based on that probability. \n",
        "- Eventually a state would be reached where assignments make sense. \n",
        "\n",
        "### LDA Pros vs Cons\n",
        "\n",
        "Pros: \n",
        "- Has been shown to produce good results over main domains\n",
        "- Effective, easy to understand tool. \n",
        "\n",
        "Cons: \n",
        "- Must know the number of topics K in advance (something that I struggled with in this project creating my own dataset) I would be forced to estimate a good number of topics for my needs. \n",
        "- Topic distribution cannot capture correlations among topics which makes it hard for me to group topics together as one. For example, linked lists and arrays are hard for me to group as simple computer science. \n",
        "- In my experience, it is a very tedious process discovering the optimal number of topics. There is not a very good explanation on the best way to determine the number of topics.  \n",
        "\n",
        "\n",
        "### Topic Modelling vs Topic Classification\n",
        "**Or LDA vs Neural Network** \n",
        "\n",
        "Difference between **identify one topic vs classify one topic**\n",
        "\n",
        "Originally, when working on this project I was under the impression I would use text classification and a neural network. However, I soon realized that this would be ineffective, based on the fact that despite having a define set of topic I would want to have returned I would not have any idea of the input text. Since I would have no idea what text or what subject the text inputted into the model would be, it would make more sense for me to try and extract the topic from any text instead of trying to classify an endless text from an endless amount of topics. If I was developing my model to be more niche in one subject, a neural network would definitely help improve my accuracy. \n",
        "\n",
        "Sources: \n",
        "\n",
        "https://www.youtube.com/watch?v=DWJYZq_fQ2A\n",
        "\n",
        "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
        "\n",
        "https://www.quora.com/Latent-Dirichlet-Allocation-LDA-What-is-the-best-way-to-determine-k-number-of-topics-in-topic-modeling\n",
        "\n",
        "https://datascience.stackexchange.com/questions/962/what-is-difference-between-text-classification-and-topic-models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVzPgbYwyUKI",
        "colab_type": "text"
      },
      "source": [
        "## Import Dataset\n",
        "In my case, I am importing a Simple Wikipedia Dataset I already preprocessed from the XML file dump Wikipedia has publicly available. Parsing code available in *parse.py*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HqsfVoEISxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open('articles/articles.txt', 'r')\n",
        "text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxXdwBwSKW80",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing \n",
        "The steps we need to do to preprocess our data is as follows: \n",
        "\n",
        "- Tokenization: \n",
        "  - Words with less than 3 characters must be removed. (Helps clear random words that may provide noise in the classifier)\n",
        "- All stop words are removed: Stop words are commonly used words like the, a, an, in that the classifier needs to be programmed to ignore to help keep noise levels in the classifier minimal. \n",
        "- Capitalization: Lowercase all the data\n",
        "- Stemming: Remove suffixes from words. \n",
        "- Lemmatization: Words in third person are changed to first person and verbs in past and future tenses are changed to present. \n",
        "- Vectorization: Convert words to vectors. Machine Learning can only read numbers so we must translate it to numbers. \n",
        "  - Types of vectorization includes: \n",
        "    - Bag of words (the technology used in this project) \n",
        "    - TFIDF\n",
        "    - Word2Vec\n",
        "    - GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6abRdPPKZgC",
        "colab_type": "code",
        "outputId": "232cb51e-102d-4304-d3bd-2166a3d2e95a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# import libraries \n",
        "import gensim\n",
        "import nltk \n",
        "import numpy as np \n",
        "\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS \n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import * \n",
        "\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrUE_tgnLGZo",
        "colab_type": "code",
        "outputId": "df32c739-79a4-4bf3-d585-3a3aa5db4018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# split each doc by a blank line\n",
        "docs_all = text.split('\\n\\n')\n",
        "print(docs_all[:5]) # print 5 of the documents"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n April is the fourth month of the year and comes between March and May It is one of four months to have day s April always starts on the same day of week as July and additionally January in leap years April always ends on the same day of the week as December April s flower s are the Sweet Pea and Asteraceae Daisy Its birthstone is the diamond The meaning of the diamond is innocence ', ' August is the eighth month of the year in the Gregorian calendar coming between July and September It has day s the same number of days as the previous month July and is named after Roman Emperor Augustus Caesar August doesn t start on the same day of the week as another month in Common year common years but starts on the same day of the week as February in Leap year leap years August always ends on the same day of the week as November ', ' Art is a creative activity by people The artist hopes that it affects the emotions of people who experience it Artists express themselves by their art Some people find art relaxing Many people disagree on how to define art Some say people are driven to make art due to their inner creativity Art includes drawing painting sculpting photography Performing arts performance art dance music poetry prose and theatre ', ' A is the first letter of the English language English alphabet The small letter a is used as a lower case vowel However the English long a omega the last letter of the Greek alphabet means the beginning and the end In musical notation the letter A is the symbol of a note in the scale below B and above G In binary numbers the letter A is A is the letter that was used to represent a team in an old TV show The A Team ', ' Administrators also called admins or sysops are users who can use special Special ListGroupRights Admin tools that help keep Wikipedia running smoothly They can use these tools because they are trusted by the community but this does not make them better or more important than anybody else The opinion of an administrator for example should not be counted as more important than the opinion of a person who chooses to change Wikipedia with an IP address just because they have administrator rights There are currently administrators ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsvniLZhSxnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use all docs for training \n",
        "# docs_train = docs_all[:60000]\n",
        "# no need for training data at all since we are using the model against unseen data\n",
        "docs_train = docs_all"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnqCu4BqS7o5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocessing \n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def lemmatize_stemming(text):\n",
        "  return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "\n",
        "# Tokenize and lemmatize\n",
        "def preprocess(text):\n",
        "  result=[]\n",
        "  for token in gensim.utils.simple_preprocess(text) :\n",
        "      if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "          result.append(lemmatize_stemming(token))\n",
        "          \n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdOuHSirTvwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# assign processed_docs to preprocessed data\n",
        "processed_docs = []\n",
        "\n",
        "for doc in docs_train: \n",
        "  processed_docs.append(preprocess(doc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atU5wmfLm9Eh",
        "colab_type": "text"
      },
      "source": [
        "### Vectorization \n",
        "**Bag of Words** \n",
        "Now we are able to vectorize the data: Vectorization is converting the words to numbers. \n",
        "\n",
        "Bag-of-words is a representation of text that describes the occurence of words in a document. Containing the following: \n",
        "1. A vocabulary of known words. \n",
        "2. A measure of the presence of known words. \n",
        "\n",
        "It is called *bag* of words because information about order or structure is discarded and the model is simply concerned whether words occur in the document and not where in the document. \n",
        "\n",
        "\n",
        "https://machinelearningmastery.com/gentle-introduction-bag-words-model/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUHvme0NVKvU",
        "colab_type": "code",
        "outputId": "4d090b5c-d268-40b9-c913-14e94dd46d2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
        "\n",
        "#  checking dictionary created\n",
        "count = 0\n",
        "for k, v in dictionary.iteritems(): \n",
        "    print(k, v)\n",
        "    count+=1\n",
        "    if count > 10: \n",
        "        break\n",
        "\n",
        "# gensim filter extremes\n",
        "# filter words that appear less than no_below\n",
        "# filter any words that do not appear in more than no_above\n",
        "dictionary.filter_extremes(no_below=0.05, no_above=0.40)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 addit\n",
            "1 april\n",
            "2 asteracea\n",
            "3 birthston\n",
            "4 come\n",
            "5 daisi\n",
            "6 decemb\n",
            "7 diamond\n",
            "8 end\n",
            "9 flower\n",
            "10 fourth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SohGq14N6k6Y",
        "colab_type": "text"
      },
      "source": [
        "## Model\n",
        "Below, we execute the LDA on our corpus. \n",
        "\n",
        "First, we assign `bow_corpus` to the bag of words version of the document. This step converts the document which is a list of words into bag of words. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9CBGKSW7I4m",
        "colab_type": "text"
      },
      "source": [
        "### Running the Model\n",
        "Parameters of `gensim.models.LdaMulticode`\n",
        "\n",
        "bow_corpus = bag of words docs file\n",
        "\n",
        "num_topics = the number of topics we want to get from our corpus. \n",
        "\n",
        "id2word = dictionary created above\n",
        "\n",
        "passes = number of passes through corpus during training \n",
        "\n",
        "iterations = max number of iterations through the corpus when inferring topic \n",
        "distribution of a corpus \n",
        "\n",
        "workers = number of worker processed to be used for parallelization\n",
        "\n",
        "More information can be found here: \n",
        "https://radimrehurek.com/gensim/models/ldamulticore.html\n",
        "\n",
        "Load into a pickle file to cache model and not need to rerun every time seperatly. \n",
        "\n",
        "**Selecting Parameters:**\n",
        "In my case I was limited in time to really expirement with tweaking all of the parameters, in the future instead of choosing something random and expirementing quickly to see what worked the best I will try and graph my results in order to see the best parameters that give me the most accurate results. Passing in an unseen document to the classifier and having no conclusive way to get the optimal number of topics really makes it a pain to tweak the parameters and becomes a game of trial and error. I ended up making this classifier with 350 topics and `model2.pkl` with 1000 topics and I am looking into expirementing with them to see what is more effective. \n",
        "\n",
        "Testing with 1000 topics expirement can be found in `/1000 topics testing`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-O6K9b9VUmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "# gensim doc2bow\n",
        "# convert document which is a list of words into bag of words\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "# running LDA on bad of words\n",
        "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
        "                                      num_topics=350,\n",
        "                                      id2word=dictionary, \n",
        "                                      passes=20,\n",
        "                                      iterations=300,\n",
        "                                      workers=3)\n",
        "\n",
        "# dump LDA model using pickle to use the model in the future without needing to rerun everything\n",
        "ldafile = open('model.pkl', 'wb')\n",
        "pickle.dump(lda_model, ldafile)\n",
        "ldafile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9T4fys1EtSQ",
        "colab_type": "text"
      },
      "source": [
        "Load picle as ```loadlda``` and save print 10 of the topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lMgtj4EVawq",
        "colab_type": "code",
        "outputId": "ae424acb-8e97-4a19-c6f9-4db4e5bce288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "pickle_in = open('model.pkl', 'rb')\n",
        "loadlda = pickle.load(pickle_in)\n",
        "\n",
        "# for each topic explore the words occuring in the topic and its relative weight \n",
        "for idx, topic in enumerate(loadlda.print_topics(num_topics=10, num_words = 10)): # print 10 of the 350 topics\n",
        "   print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
        "   print(\"\\n\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: (127, '0.165*\"drug\" + 0.145*\"travel\" + 0.052*\"fair\" + 0.050*\"illeg\" + 0.050*\"guid\" + 0.030*\"biographi\" + 0.029*\"bat\" + 0.026*\"addict\" + 0.021*\"recreat\" + 0.019*\"logo\"')\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: (270, '0.181*\"anim\" + 0.048*\"speci\" + 0.036*\"insect\" + 0.032*\"live\" + 0.031*\"hunt\" + 0.027*\"catch\" + 0.024*\"like\" + 0.022*\"predat\" + 0.021*\"fish\" + 0.019*\"egg\"')\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: (299, '0.189*\"church\" + 0.081*\"cathol\" + 0.050*\"protest\" + 0.048*\"singapor\" + 0.048*\"pope\" + 0.040*\"cathedr\" + 0.037*\"roman\" + 0.033*\"reform\" + 0.029*\"orthodox\" + 0.028*\"bishop\"')\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: (61, '0.276*\"center\" + 0.150*\"trade\" + 0.127*\"coach\" + 0.062*\"assist\" + 0.035*\"lloyd\" + 0.026*\"owen\" + 0.026*\"willi\" + 0.022*\"head\" + 0.016*\"panther\" + 0.016*\"septemb\"')\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: (298, '0.342*\"pakistan\" + 0.088*\"punjab\" + 0.075*\"surgeri\" + 0.040*\"pakistani\" + 0.031*\"lahor\" + 0.022*\"khyber\" + 0.021*\"pakhtunkhwa\" + 0.019*\"india\" + 0.018*\"surgeon\" + 0.018*\"sikh\"')\n",
            "\n",
            "\n",
            "Topic: 5 \n",
            "Words: (63, '0.182*\"centuri\" + 0.127*\"period\" + 0.110*\"hall\" + 0.060*\"fame\" + 0.055*\"earli\" + 0.043*\"revolut\" + 0.037*\"greatest\" + 0.020*\"induct\" + 0.019*\"begin\" + 0.018*\"mediev\"')\n",
            "\n",
            "\n",
            "Topic: 6 \n",
            "Words: (248, '0.156*\"sweden\" + 0.108*\"swedish\" + 0.075*\"theatr\" + 0.057*\"chess\" + 0.055*\"shore\" + 0.054*\"finland\" + 0.038*\"allen\" + 0.034*\"theater\" + 0.030*\"finnish\" + 0.026*\"lift\"')\n",
            "\n",
            "\n",
            "Topic: 7 \n",
            "Words: (11, '0.157*\"notabl\" + 0.134*\"merg\" + 0.119*\"polici\" + 0.045*\"walker\" + 0.042*\"benefit\" + 0.032*\"except\" + 0.028*\"heaven\" + 0.027*\"youtub\" + 0.026*\"hell\" + 0.023*\"guidelin\"')\n",
            "\n",
            "\n",
            "Topic: 8 \n",
            "Words: (206, '0.191*\"korea\" + 0.115*\"korean\" + 0.112*\"south\" + 0.063*\"snow\" + 0.033*\"seoul\" + 0.033*\"north\" + 0.030*\"ritual\" + 0.021*\"know\" + 0.020*\"stan\" + 0.016*\"amino\"')\n",
            "\n",
            "\n",
            "Topic: 9 \n",
            "Words: (219, '0.201*\"sourc\" + 0.111*\"galaxi\" + 0.058*\"oliv\" + 0.050*\"tiger\" + 0.050*\"milk\" + 0.041*\"earl\" + 0.039*\"beetl\" + 0.032*\"precis\" + 0.030*\"approv\" + 0.028*\"diamond\"')\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWdPw6meFD87",
        "colab_type": "text"
      },
      "source": [
        "## Testing on an unseen dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9Xhq17Oob6F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16974905-8b4e-434a-8310-527f87d1581d"
      },
      "source": [
        "def test(test): \n",
        "  bow_vector = dictionary.doc2bow(preprocess(test))\n",
        "\n",
        "  for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))\n",
        "\n",
        "\n",
        "test(\"Physics\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score: 0.501427412033081\t Topic: 0.153*\"physic\" + 0.075*\"liquid\" + 0.056*\"solid\" + 0.048*\"renam\" + 0.033*\"ridg\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF2WdC-OGDCs",
        "colab_type": "text"
      },
      "source": [
        "The model has returned the stemmed versions of some words. In order to combat this quickly, I added a spellcheck to the output. This can be fixed and improved in the future. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2OZJAjdYRX6",
        "colab_type": "code",
        "outputId": "b2a39c7b-4ac5-47cd-f859-0b0be48f7f10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# return stemmed version of word with topic spell checked\n",
        "! pip install autocorrect\n",
        "from autocorrect import Speller \n",
        "spell = Speller()\n",
        "\n",
        "def test(test): \n",
        "  bow_vector = dictionary.doc2bow(preprocess(test))\n",
        "\n",
        "  for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, spell(lda_model.print_topic(index, 5))))\n",
        "\n",
        "\n",
        "test(\"Physics\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: autocorrect in /usr/local/lib/python3.6/dist-packages (0.4.4)\n",
            "Score: 0.5013906359672546\t Topic: 0.153*\"physic\" + 0.075*\"liquid\" + 0.056*\"solid\" + 0.048*\"renal\" + 0.033*\"ring\"\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}